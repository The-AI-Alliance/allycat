{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG on HTML documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Setup Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/allycat-6/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name = MY_CONFIG.EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 00:50:33,771 [DEBUG][_create_connection]: Created new connection using: a733e3dfd02845258053e25013d61c31 (async_milvus_client.py:600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance:  workspace/rag_website_milvus.db\n"
     ]
    }
   ],
   "source": [
    "# connect to vector db\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri = MY_CONFIG.DB_URI ,\n",
    "    dim = MY_CONFIG.EMBEDDING_LENGTH , \n",
    "    collection_name = MY_CONFIG.COLLECTION_NAME,\n",
    "    overwrite=False  # so we load the index from db\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print (\"✅ Connected to Milvus instance: \", MY_CONFIG.DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Load Document Index from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded index from vector db: workspace/rag_website_milvus.db\n",
      "CPU times: user 137 ms, sys: 13.5 ms, total: 150 ms\n",
      "Wall time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, storage_context=storage_context)\n",
    "\n",
    "print (\"✅ Loaded index from vector db:\", MY_CONFIG.DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM run environment:  local_ollama\n",
      "✅ Using LLM model :  qwen3:0.6b\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.replicate import Replicate\n",
    "\n",
    "# Setup LLM\n",
    "if MY_CONFIG.LLM_RUN_ENV == 'replicate':\n",
    "    llm = Replicate(\n",
    "        model=MY_CONFIG.LLM_MODEL,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    if os.getenv('REPLICATE_API_TOKEN'):\n",
    "        print(\"✅ Found REPLICATE_API_TOKEN\")    \n",
    "    else:   \n",
    "        raise ValueError(\"❌ Please set the REPLICATE_API_TOKEN environment variable in .env file.\")\n",
    "elif MY_CONFIG.LLM_RUN_ENV == 'local_ollama':\n",
    "    llm = Ollama(\n",
    "        model= MY_CONFIG.LLM_MODEL,\n",
    "        request_timeout=30.0,\n",
    "        temperature=0.1\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"❌ Invalid LLM run environment. Please set it to 'replicate' or 'local_ollama'.\")\n",
    "print(\"✅ LLM run environment: \", MY_CONFIG.LLM_RUN_ENV)    \n",
    "print(\"✅ Using LLM model : \", MY_CONFIG.LLM_MODEL)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The AI Alliance is a collaborative effort involving members from various sectors, including non-profits, government, and academic institutions, aimed at fostering collaboration and aligning AI skills and knowledge across different fields. It seeks to improve outcomes for students and job seekers, enhance the relevance of training programs, and support more collaborative and transparent approaches between academic and corporate partners. The Alliance is growing, with a diverse range of members in 23 countries and is expanding its influence.\n"
     ]
    }
   ],
   "source": [
    "import query_utils\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('What is AI Alliance?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The main focus areas of AI Alliance, as outlined in the context, are:\n",
      "\n",
      "1. **Deploy benchmarks, tools, and other resources** to enable responsible development and use of AI systems at a global scale.\n",
      "2. **Create a catalog of vetted safety, security, and trust tools**.\n",
      "3. **Support the development and use of AI systems** through open-source tools and collaboration.\n",
      "4. **Foster a vibrant AI hardware accelerator ecosystem**.\n",
      "5. **Develop educational content and resources** to inform the public and policymakers about AI.\n",
      "6. **Launch initiatives** to encourage open development of AI in safe and beneficial ways.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('What are the main focus areas of AI Alliance?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Some AI Alliance projects include the development of frameworks for platform software such as PyTorch, Transformers, Diffusers, Kubernetes, Ray, Hugging Face, and Parameter Efficient Fine Tuning. Additionally, the AI Alliance includes the creation of open models like Llama2, Stable Diffusion, StarCoder, Bloom, and many others.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('What are some ai alliance projects?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The AI Alliance hosted the Open Source AI Demo Night in San Francisco.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('Where was the demo night held?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The AI Alliance is focused on advancing AI systems that address challenges in climate, human health, and beyond. It is also working on creating a catalog of vetted safety, security, and trust tools, supporting the development and use of AI systems, and fostering an ecosystem that promotes responsible AI development.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('What is the AI Alliance doing in the area of material science?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "To join the AI Alliance, you can visit the [contact form](https://thealliance.ai/contact) and submit a message to our membership team. Once your application is reviewed and approved, you will be invited to the AI Alliance Slack and receive additional instructions on how to join our community.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('How do I join the AI Alliance?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The information provided in the context is about AI-related projects, demos, and open-source initiatives, and there is no mention of the moon landing or its date. Therefore, based on the given context, I cannot provide a specific answer to the query about the moon landing.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('When was the moon landing?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allycat-6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
