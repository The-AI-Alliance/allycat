# ============================================
# AllyCAT GraphRAG Configuration
# ============================================
# This file contains all configuration options for AllyCAT GraphRAG.
# Copy this file to .env and customize the values.

# ============================================
# Deployment Mode
# ============================================
# Automatically run the complete pipeline on startup (Docker deployments)
# Set to true for Heroku, AWS, Google Cloud Run, etc.
AUTO_RUN_PIPELINE=false

# Memory Optimization: Remove pipeline dependencies after completion
# Saves ~350-500 MB RAM - recommended for 1GB containers (DigitalOcean, etc.)
# Set to true to automatically clean up heavy packages after pipeline completes
CLEANUP_PIPELINE_DEPS=false

# ============================================
# Website Crawling Configuration
# ============================================
# Website to crawl (required if AUTO_RUN_PIPELINE=true)
WEBSITE_URL=https://example.com
CRAWL_MAX_DOWNLOADS=100
CRAWL_MAX_DEPTH=3
WAITTIME_BETWEEN_REQUESTS=0.1

# ============================================
# LLM Configuration (Local-First with Ollama)
# ============================================
# LLM Runtime Environment
# Options: local_ollama (default for this setup), cloud
LLM_RUN_ENV=local_ollama

# LLM Model Selection
# Local: ollama/gemma3:1b (default), ollama/qwen3:0.6b
# Cloud providers: nebius/meta-llama/Meta-Llama-3.1-8B-Instruct, cerebras/llama3.1-8b
LLM_MODEL=ollama/gemma3:1b

# ============================================
# LLM API Keys (Cloud Providers)
# ============================================
# Get your API keys:
# - Nebius: https://studio.nebius.ai/ (recommended - paid)
# - Cerebras: https://cerebras.ai/ (free alternative)

NEBIUS_API_KEY=your_nebius_api_key
CEREBRAS_API_KEY=your_cerebras_api_key

# ============================================
# Local Ollama Configuration (Optional)
# ============================================
# Only needed if LLM_RUN_ENV=local_ollama
# OLLAMA_MODEL=gemma3:1b
# OLLAMA_BASE_URL=http://localhost:11434

# ============================================
# Vector Database Configuration
# ============================================
# Options: local (default for this setup), cloud_zilliz
VECTOR_DB_TYPE=local

# Zilliz Cloud Configuration (https://cloud.zilliz.com/)
ZILLIZ_CLUSTER_ENDPOINT=https://your-cluster.zilliz.cloud
ZILLIZ_TOKEN=your_zilliz_token

# Local Milvus Configuration (only if VECTOR_DB_TYPE=local)
# MILVUS_URI=./workspace/milvus_lite.db

# ============================================
# Graph Database Configuration (Neo4j)
# ============================================
# Neo4j Aura (Cloud) - Recommended: https://neo4j.com/cloud/aura/
NEO4J_URI=neo4j+s://your-instance.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password
NEO4J_DATABASE=neo4j

# Local Neo4j (only for development)
# NEO4J_URI=bolt://localhost:7687

# ============================================
# GraphRAG Extraction LLM Configuration
# ============================================
# Provider for Phase 1 entity/relationship extraction
# Default is 'cerebras' (FREE) for local setup
# Options:
#   - cerebras: Llama-3.3-70B, 65K context window (FREE - default for local)
#   - nebius: Llama-3.3-70B, 131K context window (paid, higher quality)
GRAPHRAG_LLM_PROVIDER=cerebras

# API keys (only need the one for your chosen provider)
# Get Nebius API key at: https://studio.nebius.ai/
NEBIUS_API_KEY=your_nebius_api_key_here
# Get FREE Cerebras API key at: https://cerebras.ai/
CEREBRAS_API_KEY=your_cerebras_api_key_here

# Optional: Override default models (usually not needed)
# GRAPHRAG_NEBIUS_MODEL=meta-llama/Llama-3.3-70B-Instruct
# GRAPHRAG_CEREBRAS_MODEL=llama-3.3-70b

# Optional: Override provider-specific settings (advanced users only)
# Nebius (Paid provider - default, higher context window)
# GRAPHRAG_NEBIUS_BASE_URL=https://api.studio.nebius.ai/v1/
# GRAPHRAG_NEBIUS_MAX_TOKENS=131072
# GRAPHRAG_NEBIUS_MAX_OUTPUT=8000
# GRAPHRAG_NEBIUS_RATE_DELAY=1.5
# GRAPHRAG_NEBIUS_BATCH_REDUCTION=35000

# Cerebras (FREE provider - alternative)
# GRAPHRAG_CEREBRAS_BASE_URL=https://api.cerebras.ai/v1
# GRAPHRAG_CEREBRAS_MAX_TOKENS=65536
# GRAPHRAG_CEREBRAS_MAX_OUTPUT=8000
# GRAPHRAG_CEREBRAS_RATE_DELAY=2.5
# GRAPHRAG_CEREBRAS_BATCH_REDUCTION=15000

# ============================================
# Embedding Model Configuration
# ============================================
# Embedding model for semantic search
# Options:
#   - ibm-granite/granite-embedding-30m-english (61 MB, fastest)
#   - BAAI/bge-small-en-v1.5 (129 MB, balanced)
#   - ibm-granite/granite-embedding-107m-multilingual (219 MB, multilingual)
EMBEDDING_MODEL=ibm-granite/granite-embedding-30m-english
EMBEDDING_LENGTH=384

# ============================================
# Chunking Configuration
# ============================================
CHUNK_SIZE=512
CHUNK_OVERLAP=20

# ============================================
# Graph Extraction Configuration
# ============================================
# Entity and relationship extraction parameters
GRAPH_MIN_ENTITIES=5
GRAPH_MAX_ENTITIES=15
GRAPH_MIN_RELATIONSHIPS=3
GRAPH_MAX_RELATIONSHIPS=8
GRAPH_MIN_CONFIDENCE=0.8

# Note: GRAPH_MAX_CONTENT_CHARS and GRAPH_SENTENCE_BOUNDARY_RATIO removed
# These were for old chunking-based extraction (deprecated)
# New Phase 1 uses extract_graph_full_md.py which processes full markdown files

# ============================================
# Graph Community Detection (Phase 2)
# ============================================
# Leiden algorithm parameters for community detection
GRAPH_MIN_COMMUNITY_SIZE=5
GRAPH_LEIDEN_RESOLUTION=1.0
GRAPH_LEIDEN_ITERATIONS=-1
GRAPH_LEIDEN_SEED=42
GRAPH_TARGET_COVERAGE_MIN=5.0
GRAPH_TARGET_COVERAGE_MAX=8.0
GRAPH_RESOLUTION_CANDIDATES=0.1,0.5,1.0,2.0,5.0,10.0,20.0,30.0,50.0,100.0
GRAPH_MIN_NODES_FOR_OPTIMIZATION=50

# ============================================
# Application Configuration
# ============================================
# Application type for Docker deployment
# Options: flask_graph (default), chainlit_graph, flask
APP_TYPE=flask_graph

# Flask server port
PORT=8080

# UI starter prompts (pipe-separated)
UI_STARTER_PROMPTS=What is this website?  |  What are upcoming events?  | Who are some of the partners?

# ============================================
# Port Configuration
# ============================================
# Flask apps (Vector RAG vs GraphRAG) - Auto-configured via MY_CONFIG
FLASK_VECTOR_PORT=8081      # app_flask.py (vector-only RAG)
FLASK_GRAPH_PORT=8080       # app_flask_graph.py (GraphRAG)

# Chainlit apps (interactive UI) - Default port: 8000, custom ports for Docker
CHAINLIT_VECTOR_PORT=8082   # app_chainlit.py (Docker only; native Python uses 8000)
CHAINLIT_GRAPH_PORT=8083    # app_chainlit_graph.py (Docker only; native Python uses 8000)

# Docker and external services
DOCKER_PORT=8080            # External Docker exposed port (host side)
DOCKER_APP_PORT=8080        # Internal container port (container side, matches APP_TYPE)
OLLAMA_PORT=11434           # Ollama server port (for local LLM)

# ============================================
# Workspace Configuration
# ============================================
# For native execution: use relative path 'workspace'
# For Docker: use absolute path '/allycat/workspace'
WORKSPACE_DIR=workspace

# ============================================
# Advanced Configuration
# ============================================
# Hugging Face endpoint (for Chinese users or custom mirrors)
HF_ENDPOINT=https://huggingface.co


